{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Emotion Analysis Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhxLGZ6FQyxS"
      },
      "source": [
        "# PyDrive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGypFq76QnvY"
      },
      "source": [
        "# Tweet Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBz_Xwxq2UcR"
      },
      "source": [
        "#collecting from query search\n",
        "import tweepy\n",
        "from google.colab import drive\n",
        "import json \n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "drive.mount('[your drive]')\n",
        "\n",
        "path= '[path]'\n",
        "\n",
        "#Twitter Authentication \n",
        "auth = tweepy.OAuthHandler(\"Tweepy Authenticator\")\n",
        "auth.set_access_token(\"Access Token\")\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "search_query = [\"#letfansin\"]\n",
        "\n",
        "data = []\n",
        "\n",
        "def search_tweets(search_term):\n",
        "  counter = 0\n",
        "  for tweet in tweepy.Cursor(api.search, q='\\\"{}\\\" -filter:retweets'.format(search_term), lang='en', tweet_mode='extended', result_type='recent').items(9000):\n",
        "    tweet_details = {}\n",
        "    tweet_details['name'] = tweet.user.screen_name\n",
        "    tweet_details['tweet'] = tweet.full_text\n",
        "    tweet_details['retweets'] = tweet.retweet_count\n",
        "    tweet_details['location'] = tweet.user.location\n",
        "    tweet_details['created'] = tweet.created_at.strftime(\"%d-%b-%Y\")\n",
        "    tweet_details['followers'] = tweet.user.followers_count\n",
        "    tweet_details['is_user_verified'] = tweet.user.verified\n",
        "\n",
        "    data.append(tweet_details)\n",
        "\n",
        "    counter += 1\n",
        "    if counter == 6000:\n",
        "      break\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "  with open(\"data.json\", 'w') as outfile:\n",
        "    json.dump(data, outfile)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"start\")\n",
        "  for search_term in search_query:\n",
        "    search_tweets(search_term)\n",
        "  print('done')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8hQAakR_Vp"
      },
      "source": [
        "import pandas as pd\n",
        "df_json = pd.read_json(\"/content/drive/MyDrive/CSE5095/dataset_epoch2.json\")\n",
        "df_json.to_excel(\"/content/drive/MyDrive/CSE5095/dataset_epoch2.xlsx\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BynCD1WZZupK"
      },
      "source": [
        "#collecting replies from specific tweet\n",
        "import tweepy\n",
        "from google.colab import drive\n",
        "import json \n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "drive.mount('[insert drive]')\n",
        "\n",
        "path= '[insert path]'\n",
        "\n",
        "#Twitter Authentication \n",
        "auth = tweepy.OAuthHandler(\"[inset tweepy authenticator]\")\n",
        "auth.set_access_token(\"[insert access token]\")\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "data=[]\n",
        "for full_tweets in tweepy.Cursor(api.user_timeline,screen_name=name,timeout=999999).items(10):\n",
        "  for tweet in tweepy.Cursor(api.search,'to:{} filter:replies'.format(name),result_type='recent',timeout=999999, lang='en', tweet_mode='extended').items(1000):\n",
        "    tweet_details = {}\n",
        "    tweet_details['name'] = tweet.user.screen_name\n",
        "    tweet_details['tweet'] = tweet.full_text\n",
        "    tweet_details['retweets'] = tweet.retweet_count\n",
        "    tweet_details['location'] = tweet.user.location\n",
        "    tweet_details['created'] = tweet.created_at.strftime(\"%d-%b-%Y\")\n",
        "    tweet_details['followers'] = tweet.user.followers_count\n",
        "    tweet_details['is_user_verified'] = tweet.user.verified\n",
        "    if hasattr(tweet, 'in_reply_to_status_id_str'):\n",
        "      if (tweet.in_reply_to_status_id_str==tweet_id):\n",
        "        data.append(tweet_details)   \n",
        "\n",
        "    with open(\"data.json\", 'w') as outfile:\n",
        "      json.dump(data, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw6OG9AjAlV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYpw9nzj5aCu"
      },
      "source": [
        "#Negative Emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R37SgoWL5kGm",
        "outputId": "b210584a-dc12-4a84-e105-8f1121381212"
      },
      "source": [
        "#!pip install emot\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('stopwords')\n",
        "import emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dataSet = pd.read_csv(\"/content/drive/Shareddrives/Social Media Mining/test_2.csv\")\n",
        "\n",
        "dataSet = dataSet[dataSet['neg_emo'].notna()]\n",
        "\n",
        "features = dataSet.iloc[1:,2].values\n",
        "labels = dataSet.iloc[1:,10].values\n",
        "\n",
        "processed_features = []\n",
        "\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "    return text\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stopWords = stopwords.words('english')\n",
        "  not_stopword = [word for word in text if not word in stopWords]\n",
        "  return not_stopword\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "\n",
        "    #Convert Emojis into text\n",
        "    processed_feature = convert_emojis(str(features[sentence]))\n",
        "\n",
        "    processed_feature = processed_feature.replace(\"_\", \" \")\n",
        "\n",
        "    #remove links\n",
        "    processed_feature = re.sub(r\"http\\S+\", \"\", processed_feature)\n",
        "\n",
        "    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_feature = lemmatize_sentence(processed_feature)\n",
        "\n",
        "    processed_features.append(processed_feature)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction import text \n",
        "\n",
        "\n",
        "my_additional_stop_words = (['heart','letfansin', 'club', 'face', 'people', 'like', 'goal', 'circle' , 'circlewhite', 'circlered', 'northern', 'ireland', 'league', 'team', 'floor', 'laugh', 'talksportdrive', 'klfcofficial' , 'skin', 'tone', 'game', 'engwal', 'male', 'female', 'cc', 'soccer', 'fawales', 'fa', 'ball'])\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=4, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words))\n",
        "processed_features_tfidf = vectorizer.fit_transform(processed_features).toarray()\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "#oversample using SMOTE\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "strategy = {'sad':600, 'ang':600, 'fea':200}\n",
        "oversample = SMOTE(sampling_strategy= strategy)\n",
        "undersample= TomekLinks(\"not minority\")\n",
        "\n",
        "processed_features_tfidf, labels = oversample.fit_resample(processed_features_tfidf, labels)\n",
        "processed_features_tfidf, labels = undersample.fit_resample(processed_features_tfidf, labels)\n",
        "\n",
        "#train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features_tfidf, labels, test_size=0.2, random_state=8)\n",
        "\n",
        "\n",
        "#import cross_val, metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "\n",
        "print(\"Random Forest Scores:\\n\")\n",
        "print(classification_report(y_test,predictions))\n",
        "#print(\"Accuracy Score:\\n\")\n",
        "print(\"5-fold Cross Validation Accuracy Scores:\\n\")\n",
        "print(accuracy_score(y_test, predictions))\n",
        "#print(\"5-fold Cross Validation Accuracy Scores:\\n\", cross_val.mean(), \"\\n\")\n",
        "\n",
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "text_classifier = LogisticRegression(random_state=166)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Scores:\\n\")\n",
        "print(classification_report(y_test, predictions))\n",
        "#print(\"Accuracy Score:\\n\")\n",
        "print(\"5-fold Cross Validation Accuracy Scores:\\n\")\n",
        "print(accuracy_score(y_test, predictions))\n",
        "#print(\"5-fold Cross Validation Accuracy Scores:\\n\", cross_val.mean(), \"\\n\")\n",
        "\n",
        "#LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_classifier = LinearSVC()\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "print(\"LinearSVC Scores:\\n\")\n",
        "print(classification_report(y_test, predictions))\n",
        "#print(\"Accuracy Score:\\n\")\n",
        "print(\"5-fold Cross Validation Accuracy Scores:\\n\")\n",
        "print(accuracy_score(y_test, predictions))\n",
        "#print(\"5-fold Cross Validation Accuracy Scores:\\n\", cross_val.mean(), \"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest Scores:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.81      0.63      0.71       117\n",
            "         dis       0.70      0.90      0.79       213\n",
            "         fea       1.00      0.84      0.91        31\n",
            "         sad       0.84      0.65      0.73       124\n",
            "\n",
            "    accuracy                           0.77       485\n",
            "   macro avg       0.84      0.76      0.79       485\n",
            "weighted avg       0.78      0.77      0.76       485\n",
            "\n",
            "5-fold Cross Validation Accuracy Scores:\n",
            "\n",
            "0.7670103092783506\n",
            "Logistic Regression Scores:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.76      0.50      0.60       117\n",
            "         dis       0.67      0.91      0.77       213\n",
            "         fea       0.92      0.71      0.80        31\n",
            "         sad       0.88      0.68      0.77       124\n",
            "\n",
            "    accuracy                           0.74       485\n",
            "   macro avg       0.81      0.70      0.73       485\n",
            "weighted avg       0.76      0.74      0.73       485\n",
            "\n",
            "5-fold Cross Validation Accuracy Scores:\n",
            "\n",
            "0.7381443298969073\n",
            "LinearSVC Scores:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.70      0.66      0.68       117\n",
            "         dis       0.76      0.81      0.79       213\n",
            "         fea       0.94      0.94      0.94        31\n",
            "         sad       0.84      0.79      0.81       124\n",
            "\n",
            "    accuracy                           0.78       485\n",
            "   macro avg       0.81      0.80      0.80       485\n",
            "weighted avg       0.78      0.78      0.78       485\n",
            "\n",
            "5-fold Cross Validation Accuracy Scores:\n",
            "\n",
            "0.777319587628866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhHpC9u35sUP"
      },
      "source": [
        "#random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "print(\"Random Forest Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")\n",
        "\n",
        "#multinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "text_classifier = MultinomialNB()\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "print(\"mNB Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(classification_report(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")\n",
        "\n",
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "text_classifier = LogisticRegression(random_state=0)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(classification_report(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")\n",
        "\n",
        "#LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_classifier = LinearSVC()\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "print(\"LinearSVC Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(classification_report(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiQVfH8ooG0L"
      },
      "source": [
        "#train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features_tfidf, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "#import cross_val, metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "#print(\"Random Forest Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(\"Random Forest Cross Validation Accuracy Scores:\\n\")\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")\n",
        "\n",
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "text_classifier = LogisticRegression(random_state=0)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "\n",
        "#print(\"Logistic Regression Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(\"Logistic Regression Cross Validation Accuracy Scores:\\n\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")\n",
        "\n",
        "#LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_classifier = LinearSVC()\n",
        "text_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "cross_val = cross_val_score(text_classifier, processed_features_tfidf, labels, scoring='accuracy')\n",
        "\n",
        "#print(\"LinearSVC Cross Validation Score:\\n\", cross_val.mean())\n",
        "print(\"LinearSVC Cross Validation Scores:\\n\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxKXl4tyUn5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}